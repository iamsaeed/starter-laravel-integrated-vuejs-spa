APP_NAME=Laravel
APP_ENV=local
APP_KEY=
APP_DEBUG=true
APP_URL=http://localhost
FRONTEND_URL=http://localhost:5173

APP_LOCALE=en
APP_FALLBACK_LOCALE=en
APP_FAKER_LOCALE=en_US

APP_MAINTENANCE_DRIVER=file
# APP_MAINTENANCE_STORE=database

PHP_CLI_SERVER_WORKERS=4

BCRYPT_ROUNDS=12

LOG_CHANNEL=stack
LOG_STACK=single
LOG_DEPRECATIONS_CHANNEL=null
LOG_LEVEL=debug

DB_CONNECTION=sqlite
# DB_HOST=127.0.0.1
# DB_PORT=3306
# DB_DATABASE=laravel
# DB_USERNAME=root
# DB_PASSWORD=

SESSION_DRIVER=database
SESSION_LIFETIME=120
SESSION_ENCRYPT=false
SESSION_PATH=/
SESSION_DOMAIN=null

BROADCAST_CONNECTION=log
FILESYSTEM_DISK=local
QUEUE_CONNECTION=database

CACHE_STORE=database
# CACHE_PREFIX=

MEMCACHED_HOST=127.0.0.1

REDIS_CLIENT=phpredis
REDIS_HOST=127.0.0.1
REDIS_PASSWORD=null
REDIS_PORT=6379

# AI Configuration
VITE_OPENAI_API_KEY=
OPENAI_ORGANIZATION=
AI_PROVIDER=openai
AI_MODEL=gpt-4o-mini

# Optional AI Providers
ANTHROPIC_API_KEY=
OLLAMA_BASE_URL=http://localhost:11434

# Monitoring (Optional)
NEURON_MONITORING=false
INSPECTOR_INGESTION_KEY=

# AI Intent Classification
# Enable AI-powered intent detection for better tool selection
NEURON_USE_AI_INTENT=true
# Use hybrid mode (keywords for obvious cases, AI for ambiguous)
NEURON_INTENT_HYBRID=true
# Log AI intent decisions for analysis (set to true for debugging)
NEURON_LOG_INTENT=false

MAIL_MAILER=log
MAIL_SCHEME=null
MAIL_HOST=127.0.0.1
MAIL_PORT=2525
MAIL_USERNAME=null
MAIL_PASSWORD=null
MAIL_FROM_ADDRESS="hello@example.com"
MAIL_FROM_NAME="${APP_NAME}"

AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_DEFAULT_REGION=us-east-1
AWS_BUCKET=
AWS_USE_PATH_STYLE_ENDPOINT=false

# Digital Ocean Spaces Configuration (S3-compatible storage)
DO_SPACES_KEY=
DO_SPACES_SECRET=
DO_SPACES_REGION=blr1
DO_SPACES_BUCKET=your-bucket-name
DO_SPACES_ENDPOINT=https://blr1.digitaloceanspaces.com
DO_SPACES_URL=https://your-bucket-name.blr1.digitaloceanspaces.com
DO_SPACES_CDN_URL=https://your-cdn-endpoint.cdn.digitaloceanspaces.com

VITE_APP_NAME="${APP_NAME}"

# AI Provider Configuration
# Default LLM provider to use for all AI agents
# Supported: openai, anthropic, gemini, deepseek, mistral, xai, ollama, aws,
#            groq, together, fireworks, perplexity, openrouter, deepinfra
AI_PROVIDER=deepseek

# Per-Agent Provider/Model Overrides (Optional)
# You can override the provider or model for specific agents while keeping the default for others
#
# Available agents: code, research, supervisor, conversation, expense,
#                   module, analysis, email_drafting, blog_writing, search_engine, email_design
#
# Example: Use Anthropic for research agent while keeping OpenAI as default for others
# RESEARCH_AGENT_PROVIDER=anthropic
# RESEARCH_AGENT_MODEL=claude-3-5-sonnet-20241022
#
# Example: Use GPT-4o for code agent while other agents use the default model
# CODE_AGENT_MODEL=gpt-4o
#
# Example: Use different providers for different agents
# CODE_AGENT_PROVIDER=openai
# CODE_AGENT_MODEL=gpt-4o
# RESEARCH_AGENT_PROVIDER=anthropic
# RESEARCH_AGENT_MODEL=claude-3-5-sonnet-20241022
# SUPERVISOR_AGENT_PROVIDER=gemini
# SUPERVISOR_AGENT_MODEL=gemini-2.0-flash-thinking-exp-01-21

# OpenAI Configuration
VITE_OPENAI_API_KEY=
VITE_OPENAI_MODEL=gpt-4o-mini
VITE_OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Anthropic (Claude) Configuration
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
ANTHROPIC_VERSION=2023-06-01
ANTHROPIC_MAX_TOKENS=8192

# Google Gemini Configuration
VITE_GEMINI_API_KEY=
VITE_GEMINI_MODEL=gemini-2.0-flash-exp

# Deepseek Configuration
VITE_DEEPSEEK_API_KEY=
VITE_DEEPSEEK_MODEL=deepseek-chat
# DEEPSEEK_BASE_URL=  # Optional: for custom endpoints (e.g., DigitalOcean AI)
# Example with DigitalOcean AI:
# DEEPSEEK_BASE_URL=https://o23obf2uljxbyzc3t5zn2hmg.agents.do-ai.run
# VITE_DEEPSEEK_MODEL="DeepSeek R1 Distill Llama 70B"

# Mistral Configuration
MISTRAL_API_KEY=
MISTRAL_MODEL=mistral-large-latest

# XAI (Grok) Configuration
XAI_API_KEY=
XAI_MODEL=grok-beta

# Ollama Configuration (for local models)
OLLAMA_API_KEY=ollama
OLLAMA_MODEL=llama2
OLLAMA_BASE_URL=http://localhost:11434

# AWS Bedrock Configuration
# AWS_ACCESS_KEY_ID= # Already defined above
# AWS_SECRET_ACCESS_KEY= # Already defined above
AWS_BEDROCK_REGION=us-east-1
AWS_BEDROCK_MODEL=anthropic.claude-3-sonnet-20240229-v1:0

# OpenAI-Compatible API Providers
# These providers offer OpenAI-compatible APIs for easy integration

# Groq Configuration (ultra-fast inference)
GROQ_API_KEY=
GROQ_BASE_URL=https://api.groq.com/openai/v1
GROQ_MODEL=llama-3.3-70b-versatile

# Together.ai Configuration (open-source models)
TOGETHER_API_KEY=
TOGETHER_BASE_URL=https://api.together.xyz/v1
TOGETHER_MODEL=meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo

# Fireworks.ai Configuration (fast inference)
FIREWORKS_API_KEY=
FIREWORKS_BASE_URL=https://api.fireworks.ai/inference/v1
FIREWORKS_MODEL=accounts/fireworks/models/llama-v3p1-70b-instruct

# Perplexity Configuration (with online search)
PERPLEXITY_API_KEY=
PERPLEXITY_BASE_URL=https://api.perplexity.ai
PERPLEXITY_MODEL=llama-3.1-sonar-large-128k-online

# OpenRouter Configuration (aggregator - access to multiple models)
OPENROUTER_API_KEY=
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_MODEL=anthropic/claude-3.5-sonnet

# DeepInfra Configuration (affordable cloud inference)
DEEPINFRA_API_KEY=
DEEPINFRA_BASE_URL=https://api.deepinfra.com/v1/openai
DEEPINFRA_MODEL=meta-llama/Meta-Llama-3.1-70B-Instruct

# Qdrant Configuration (optional - for vector database/RAG)
VITE_QDRANT_HOST=http://localhost
VITE_QDRANT_PORT=6333
VITE_QDRANT_API_KEY=

# SERP API Configuration (optional - for web search in SearchEngine and Research agents)
# Get your API key from https://serpapi.com/
VITE_SERP_API_KEY=
VITE_SERP_ENGINE=google
